"""
PRO-GRADE AUTO-CAPTION SYSTEM FOR VIDEO ADS (2025 Edition)
Uses latest Whisper models with advanced AI features.

NOVEMBER 2025 UPGRADES:
- Whisper Large V3 Turbo (8x faster, same accuracy)
- Distil-Whisper for ultra-fast previews (6x faster)
- OpenAI Whisper API fallback (cloud-based)
- Real-time transcription streaming
- Enhanced speaker diarization (pyannote 3.1)
- Word-level alignment with precise timing
- Multi-language with auto-translation
- GPU memory optimization
- Batch processing for multiple videos
- Queue management for large files

Features:
- OpenAI Whisper integration (multiple model sizes including V3 Turbo)
- GPU acceleration for fast transcription
- Multiple language support with auto-detection
- Word-level timestamps for animated captions
- Speaker diarization (who is speaking)
- Custom vocabulary for fitness terms
- Profanity filtering
- Multiple caption styles (Instagram, YouTube, Karaoke, TikTok, Hormozi)
- SRT/VTT file generation
- Burn-in captions with FFmpeg drawtext
- Custom fonts, colors, backgrounds
- Emoji support
- Auto-punctuation
- Caption timing optimization
"""

import os
import json
import re
import subprocess
import tempfile
import asyncio
import gc
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any, AsyncIterator, Union
from dataclasses import dataclass, asdict, field
from enum import Enum
import logging
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from queue import Queue
import threading

# Whisper and ML libraries - support multiple backends
import torch
import numpy as np

# OpenAI Whisper (original)
try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    logging.warning("openai-whisper not installed. Original Whisper disabled.")

# Faster Whisper (optimized with CTranslate2)
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False
    logging.warning("faster-whisper not installed. Optimized inference disabled.")

# Transformers for Whisper V3 Turbo and Distil-Whisper
try:
    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("transformers not installed. Whisper V3 Turbo disabled.")

# OpenAI API for cloud fallback
try:
    import openai
    OPENAI_API_AVAILABLE = True
except ImportError:
    OPENAI_API_AVAILABLE = False
    logging.warning("openai not installed. OpenAI API fallback disabled.")

# For speaker diarization (optional)
try:
    from pyannote.audio import Pipeline
    DIARIZATION_AVAILABLE = True
except ImportError:
    DIARIZATION_AVAILABLE = False
    logging.warning("pyannote.audio not installed. Speaker diarization disabled.")

# For profanity filtering
try:
    from better_profanity import profanity
    profanity.load_censor_words()
    PROFANITY_FILTER_AVAILABLE = True
except ImportError:
    PROFANITY_FILTER_AVAILABLE = False
    logging.warning("better_profanity not installed. Profanity filtering disabled.")

# For translation
try:
    from deep_translator import GoogleTranslator
    TRANSLATION_AVAILABLE = True
except ImportError:
    TRANSLATION_AVAILABLE = False
    logging.warning("deep-translator not installed. Auto-translation disabled.")


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class WhisperBackend(Enum):
    """Whisper backend types."""
    OPENAI = "openai"              # Original OpenAI Whisper
    FASTER_WHISPER = "faster"      # Faster Whisper with CTranslate2
    TRANSFORMERS = "transformers"  # HuggingFace Transformers (V3 Turbo)
    OPENAI_API = "api"            # OpenAI Cloud API


class WhisperModelSize(Enum):
    """Available Whisper model sizes (November 2025)."""
    # Standard models
    TINY = "tiny"
    BASE = "base"
    SMALL = "small"
    MEDIUM = "medium"
    LARGE = "large"
    LARGE_V2 = "large-v2"
    LARGE_V3 = "large-v3"

    # NEW: Whisper Large V3 Turbo (November 2025) - 8x faster!
    LARGE_V3_TURBO = "large-v3-turbo"

    # NEW: Distil-Whisper models (ultra-fast, good for previews)
    DISTIL_LARGE_V2 = "distil-large-v2"
    DISTIL_MEDIUM_EN = "distil-medium.en"
    DISTIL_SMALL_EN = "distil-small.en"


class TranscriptionMode(Enum):
    """Transcription processing modes."""
    FULL = "full"              # Full accuracy, slower
    FAST = "fast"              # Fast preview with Distil-Whisper
    REALTIME = "realtime"      # Real-time streaming
    BATCH = "batch"            # Batch processing for multiple files


class CaptionStyle(Enum):
    """Available caption styles for video ads."""
    INSTAGRAM = "instagram"  # Word-by-word pop with color highlight
    YOUTUBE = "youtube"      # Sentence blocks at bottom
    KARAOKE = "karaoke"      # Word highlight as spoken
    TIKTOK = "tiktok"        # Centered, bold, animated
    HORMOZI = "hormozi"      # Big bold words, one at a time


@dataclass
class Word:
    """Individual word with timing information."""
    text: str
    start: float
    end: float
    confidence: float
    speaker: Optional[str] = None


@dataclass
class Caption:
    """Caption segment with timing and styling."""
    text: str
    start: float
    end: float
    words: List[Word]
    speaker: Optional[str] = None


@dataclass
class CaptionStyleConfig:
    """Configuration for caption styling."""
    # Font settings
    font_family: str = "Arial-Bold"
    font_size: int = 48
    font_color: str = "white"
    highlight_color: str = "yellow"

    # Background/outline settings
    box_color: str = "black@0.6"
    border_width: int = 2
    border_color: str = "black"

    # Position settings
    position_x: str = "(w-text_w)/2"  # Centered horizontally
    position_y: str = "h-th-50"        # Near bottom

    # Animation settings
    animate: bool = True
    duration_per_word: float = 0.3

    # Style-specific settings
    all_caps: bool = False
    max_chars_per_line: int = 40
    max_words_per_line: int = 6

    # Emoji settings
    emoji_support: bool = True

    # Shadow settings
    shadow_enabled: bool = True
    shadow_color: str = "black@0.8"
    shadow_x: int = 2
    shadow_y: int = 2


class FitnessVocabulary:
    """Custom vocabulary for fitness terms."""

    TERMS = {
        # Common misspellings/misheard terms
        "rep": ["reps", "repetition", "repetitions"],
        "set": ["sets"],
        "workout": ["work out", "workouts"],
        "cardio": ["cardiovascular"],
        "gains": ["gain", "muscle gains"],
        "protein": ["proteins", "protein shake"],
        "macro": ["macros", "macronutrients"],
        "calorie": ["calories", "cal", "kcal"],
        "cut": ["cutting", "cuts"],
        "bulk": ["bulking", "bulks"],
        "shred": ["shredded", "shredding"],
        "pump": ["pumped", "pumping"],
        "PR": ["personal record", "P.R.", "pr"],
        "HIIT": ["high intensity interval training", "H.I.I.T."],
        "BMI": ["body mass index", "B.M.I."],
        "BMR": ["basal metabolic rate", "B.M.R."],
        "TDEE": ["total daily energy expenditure", "T.D.E.E."],
    }

    @classmethod
    def enhance_text(cls, text: str) -> str:
        """Enhance text with proper fitness terminology."""
        enhanced = text
        for key, variations in cls.TERMS.items():
            for variation in variations:
                # Case-insensitive replacement
                pattern = re.compile(re.escape(variation), re.IGNORECASE)
                enhanced = pattern.sub(key, enhanced)
        return enhanced


class ProfanityFilter:
    """Filter profanity from captions."""

    @staticmethod
    def filter_text(text: str, replacement: str = "***") -> str:
        """Filter profanity from text."""
        if not PROFANITY_FILTER_AVAILABLE:
            return text
        return profanity.censor(text, replacement)

    @staticmethod
    def contains_profanity(text: str) -> bool:
        """Check if text contains profanity."""
        if not PROFANITY_FILTER_AVAILABLE:
            return False
        return profanity.contains_profanity(text)


class GPUMemoryManager:
    """
    Manage GPU memory for optimal performance.
    Prevents OOM errors during transcription.
    """

    @staticmethod
    def get_available_memory() -> float:
        """Get available GPU memory in GB."""
        if not torch.cuda.is_available():
            return 0.0

        torch.cuda.empty_cache()
        gpu_mem = torch.cuda.get_device_properties(0).total_memory
        gpu_mem_allocated = torch.cuda.memory_allocated(0)
        available = (gpu_mem - gpu_mem_allocated) / (1024 ** 3)  # Convert to GB

        return available

    @staticmethod
    def clear_cache():
        """Clear GPU cache to free memory."""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()

    @staticmethod
    def optimize_batch_size(video_duration: float, model_size: str) -> int:
        """
        Calculate optimal batch size based on GPU memory and video duration.

        Args:
            video_duration: Duration in seconds
            model_size: Model size name

        Returns:
            Optimal batch size
        """
        available_mem = GPUMemoryManager.get_available_memory()

        # Memory requirements per model (approximate GB)
        mem_requirements = {
            "tiny": 1,
            "base": 1,
            "small": 2,
            "medium": 5,
            "large": 10,
            "large-v2": 10,
            "large-v3": 10,
            "large-v3-turbo": 6,  # More efficient!
            "distil-large-v2": 4,
            "distil-medium.en": 2,
            "distil-small.en": 1,
        }

        required_mem = mem_requirements.get(model_size, 5)

        # Calculate batch size
        if available_mem < required_mem:
            logger.warning(f"Low GPU memory: {available_mem:.1f}GB. May need to use CPU.")
            return 1

        # More memory = larger batches
        batch_size = max(1, int(available_mem / required_mem))

        return min(batch_size, 8)  # Cap at 8


class TranscriptionQueue:
    """
    Queue management for large transcription jobs.
    Handles multiple videos efficiently.
    """

    def __init__(self, max_workers: int = 4):
        """
        Initialize transcription queue.

        Args:
            max_workers: Maximum parallel workers
        """
        self.queue = Queue()
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.results = {}
        self.lock = threading.Lock()

    def add_job(self, job_id: str, video_path: str, **kwargs) -> str:
        """
        Add transcription job to queue.

        Args:
            job_id: Unique job identifier
            video_path: Path to video file
            **kwargs: Additional transcription parameters

        Returns:
            Job ID
        """
        job = {
            "id": job_id,
            "video_path": video_path,
            "status": "queued",
            "kwargs": kwargs
        }

        self.queue.put(job)
        logger.info(f"Job {job_id} added to queue. Queue size: {self.queue.qsize()}")

        return job_id

    def process_queue(self, transcriber):
        """
        Process all jobs in queue.

        Args:
            transcriber: WhisperTranscriber instance
        """
        while not self.queue.empty():
            job = self.queue.get()

            try:
                job["status"] = "processing"
                logger.info(f"Processing job {job['id']}")

                # Transcribe
                result = transcriber.transcribe(
                    job["video_path"],
                    **job["kwargs"]
                )

                with self.lock:
                    self.results[job["id"]] = {
                        "status": "completed",
                        "result": result
                    }

                logger.info(f"Job {job['id']} completed")

            except Exception as e:
                logger.error(f"Job {job['id']} failed: {str(e)}")
                with self.lock:
                    self.results[job["id"]] = {
                        "status": "failed",
                        "error": str(e)
                    }

            finally:
                self.queue.task_done()

    def get_result(self, job_id: str) -> Optional[Dict]:
        """Get result for a job."""
        with self.lock:
            return self.results.get(job_id)


class LanguageDetector:
    """
    Enhanced language detection and translation.
    """

    # Language codes to names
    LANGUAGES = {
        "en": "English",
        "es": "Spanish",
        "fr": "French",
        "de": "German",
        "it": "Italian",
        "pt": "Portuguese",
        "ru": "Russian",
        "ja": "Japanese",
        "ko": "Korean",
        "zh": "Chinese",
        "ar": "Arabic",
        "hi": "Hindi",
    }

    @staticmethod
    def detect_language(transcription_result: Dict) -> str:
        """
        Detect language from Whisper transcription.

        Args:
            transcription_result: Whisper transcription result

        Returns:
            Language code (e.g., 'en', 'es')
        """
        return transcription_result.get("language", "en")

    @staticmethod
    def translate_text(text: str, source_lang: str, target_lang: str) -> str:
        """
        Translate text to target language.

        Args:
            text: Text to translate
            source_lang: Source language code
            target_lang: Target language code

        Returns:
            Translated text
        """
        if not TRANSLATION_AVAILABLE:
            logger.warning("Translation not available. Returning original text.")
            return text

        if source_lang == target_lang:
            return text

        try:
            translator = GoogleTranslator(source=source_lang, target=target_lang)
            translated = translator.translate(text)
            return translated
        except Exception as e:
            logger.error(f"Translation error: {str(e)}")
            return text

    @staticmethod
    def translate_captions(
        captions: List['Caption'],
        source_lang: str,
        target_lang: str
    ) -> List['Caption']:
        """
        Translate all captions to target language.

        Args:
            captions: List of Caption objects
            source_lang: Source language code
            target_lang: Target language code

        Returns:
            List of translated Caption objects
        """
        translated_captions = []

        for caption in captions:
            translated_text = LanguageDetector.translate_text(
                caption.text,
                source_lang,
                target_lang
            )

            # Create new caption with translated text
            translated_caption = Caption(
                text=translated_text,
                start=caption.start,
                end=caption.end,
                words=caption.words,  # Keep original timing
                speaker=caption.speaker
            )

            translated_captions.append(translated_caption)

        return translated_captions


class WhisperTranscriber:
    """
    UPGRADED: Multi-backend Whisper transcriber (2025 Edition).

    Supports:
    - Whisper Large V3 Turbo (8x faster via Transformers)
    - Distil-Whisper (ultra-fast previews)
    - Faster-Whisper (CTranslate2 optimization)
    - OpenAI API (cloud fallback)
    - Original OpenAI Whisper
    """

    def __init__(
        self,
        model_size: WhisperModelSize = WhisperModelSize.LARGE_V3_TURBO,
        backend: WhisperBackend = WhisperBackend.TRANSFORMERS,
        device: Optional[str] = None,
        language: Optional[str] = None,
        compute_type: str = "float16",
        mode: TranscriptionMode = TranscriptionMode.FULL,
        openai_api_key: Optional[str] = None
    ):
        """
        Initialize Whisper transcriber with backend selection.

        Args:
            model_size: Whisper model size to use (defaults to V3 Turbo!)
            backend: Backend to use (transformers, faster, openai, api)
            device: Device to use ('cuda' or 'cpu'). Auto-detect if None.
            language: Language code (e.g., 'en', 'es'). Auto-detect if None.
            compute_type: Computation precision ('float16' or 'int8' for faster inference)
            mode: Transcription mode (full, fast, realtime, batch)
            openai_api_key: OpenAI API key for cloud fallback
        """
        self.model_size = model_size
        self.backend = backend
        self.language = language
        self.compute_type = compute_type
        self.mode = mode
        self.openai_api_key = openai_api_key or os.environ.get("OPENAI_API_KEY")

        # Auto-detect device
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        # Auto-select backend if needed
        self.backend = self._select_best_backend()

        logger.info(f"Loading Whisper model: {model_size.value} on {self.device} using {self.backend.value} backend")

        # Load model based on backend
        self.model = None
        self.processor = None

        if self.backend == WhisperBackend.TRANSFORMERS:
            self._load_transformers_model()
        elif self.backend == WhisperBackend.FASTER_WHISPER:
            self._load_faster_whisper_model()
        elif self.backend == WhisperBackend.OPENAI:
            self._load_openai_whisper_model()
        elif self.backend == WhisperBackend.OPENAI_API:
            self._setup_openai_api()

        logger.info(f"Whisper model loaded successfully")

        # Clear GPU cache after loading
        GPUMemoryManager.clear_cache()

    def _select_best_backend(self) -> WhisperBackend:
        """
        Auto-select the best available backend for the model.

        Returns:
            Best available backend
        """
        # V3 Turbo and Distil-Whisper need Transformers
        if self.model_size in [
            WhisperModelSize.LARGE_V3_TURBO,
            WhisperModelSize.DISTIL_LARGE_V2,
            WhisperModelSize.DISTIL_MEDIUM_EN,
            WhisperModelSize.DISTIL_SMALL_EN
        ]:
            if TRANSFORMERS_AVAILABLE:
                return WhisperBackend.TRANSFORMERS
            else:
                logger.warning(f"{self.model_size.value} requires transformers. Falling back.")

        # Prefer Faster-Whisper for standard models (it's faster!)
        if FASTER_WHISPER_AVAILABLE and self.backend == WhisperBackend.FASTER_WHISPER:
            return WhisperBackend.FASTER_WHISPER

        # Fall back to original Whisper
        if WHISPER_AVAILABLE:
            return WhisperBackend.OPENAI

        # Last resort: OpenAI API
        if OPENAI_API_AVAILABLE and self.openai_api_key:
            logger.warning("No local Whisper available. Using OpenAI API.")
            return WhisperBackend.OPENAI_API

        raise RuntimeError("No Whisper backend available! Install openai-whisper or transformers.")

    def _load_transformers_model(self):
        """Load Whisper model via HuggingFace Transformers (for V3 Turbo and Distil-Whisper)."""
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("transformers not installed. Install with: pip install transformers")

        # Model mapping
        model_names = {
            WhisperModelSize.LARGE_V3_TURBO: "openai/whisper-large-v3-turbo",
            WhisperModelSize.DISTIL_LARGE_V2: "distil-whisper/distil-large-v2",
            WhisperModelSize.DISTIL_MEDIUM_EN: "distil-whisper/distil-medium.en",
            WhisperModelSize.DISTIL_SMALL_EN: "distil-whisper/distil-small.en",
            WhisperModelSize.LARGE_V3: "openai/whisper-large-v3",
            WhisperModelSize.LARGE_V2: "openai/whisper-large-v2",
            WhisperModelSize.MEDIUM: "openai/whisper-medium",
            WhisperModelSize.SMALL: "openai/whisper-small",
            WhisperModelSize.BASE: "openai/whisper-base",
            WhisperModelSize.TINY: "openai/whisper-tiny",
        }

        model_name = model_names.get(self.model_size, "openai/whisper-large-v3-turbo")

        logger.info(f"Loading Transformers model: {model_name}")

        # Load model with optimal settings
        torch_dtype = torch.float16 if self.device == "cuda" else torch.float32

        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(
            model_name,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
            use_safetensors=True
        )

        self.model.to(self.device)

        # Load processor
        self.processor = AutoProcessor.from_pretrained(model_name)

        # Create pipeline for easy inference
        self.pipe = pipeline(
            "automatic-speech-recognition",
            model=self.model,
            tokenizer=self.processor.tokenizer,
            feature_extractor=self.processor.feature_extractor,
            max_new_tokens=128,
            chunk_length_s=30,
            batch_size=GPUMemoryManager.optimize_batch_size(30, self.model_size.value),
            return_timestamps=True,
            torch_dtype=torch_dtype,
            device=self.device
        )

    def _load_faster_whisper_model(self):
        """Load Faster-Whisper model (CTranslate2 optimized)."""
        if not FASTER_WHISPER_AVAILABLE:
            raise ImportError("faster-whisper not installed. Install with: pip install faster-whisper")

        # Faster-Whisper doesn't support Distil models
        if "distil" in self.model_size.value:
            logger.warning(f"Faster-Whisper doesn't support {self.model_size.value}. Using Transformers.")
            self.backend = WhisperBackend.TRANSFORMERS
            self._load_transformers_model()
            return

        model_name = self.model_size.value

        # Map V3 Turbo to V3 for faster-whisper (it doesn't have turbo yet)
        if model_name == "large-v3-turbo":
            model_name = "large-v3"
            logger.info("Using large-v3 for Faster-Whisper (turbo not yet supported)")

        logger.info(f"Loading Faster-Whisper model: {model_name}")

        self.model = WhisperModel(
            model_name,
            device=self.device,
            compute_type=self.compute_type
        )

    def _load_openai_whisper_model(self):
        """Load original OpenAI Whisper model."""
        if not WHISPER_AVAILABLE:
            raise ImportError("openai-whisper not installed. Install with: pip install openai-whisper")

        # Original Whisper doesn't support new models
        if self.model_size in [
            WhisperModelSize.LARGE_V3_TURBO,
            WhisperModelSize.DISTIL_LARGE_V2,
            WhisperModelSize.DISTIL_MEDIUM_EN,
            WhisperModelSize.DISTIL_SMALL_EN
        ]:
            logger.warning(f"{self.model_size.value} not available in openai-whisper. Using large-v3.")
            model_name = "large-v3"
        else:
            model_name = self.model_size.value

        logger.info(f"Loading OpenAI Whisper model: {model_name}")

        self.model = whisper.load_model(model_name, device=self.device)

    def _setup_openai_api(self):
        """Setup OpenAI API for cloud transcription."""
        if not OPENAI_API_AVAILABLE:
            raise ImportError("openai not installed. Install with: pip install openai")

        if not self.openai_api_key:
            raise ValueError("OpenAI API key required for API backend. Set OPENAI_API_KEY environment variable.")

        openai.api_key = self.openai_api_key
        logger.info("OpenAI API configured for cloud transcription")

    def transcribe(
        self,
        audio_path: str,
        word_timestamps: bool = True,
        initial_prompt: Optional[str] = None,
        temperature: float = 0.0,
        best_of: int = 5,
        beam_size: int = 5,
        patience: float = 1.0,
        suppress_tokens: Optional[List[int]] = None,
        condition_on_previous_text: bool = True,
        fp16: bool = True
    ) -> Dict[str, Any]:
        """
        Transcribe audio file using Whisper (multi-backend support).

        Args:
            audio_path: Path to audio/video file
            word_timestamps: Enable word-level timestamps
            initial_prompt: Optional prompt to guide the model
            temperature: Sampling temperature (0 = deterministic)
            best_of: Number of candidates to generate
            beam_size: Beam search size
            patience: Beam search patience
            suppress_tokens: Token IDs to suppress
            condition_on_previous_text: Use previous text as context
            fp16: Use FP16 precision

        Returns:
            Dictionary with transcription results including word-level timestamps
        """
        logger.info(f"Transcribing: {audio_path} using {self.backend.value} backend")

        # Check if GPU is available and adjust fp16
        if self.device == "cpu":
            fp16 = False

        # Route to appropriate backend
        if self.backend == WhisperBackend.TRANSFORMERS:
            result = self._transcribe_transformers(audio_path, word_timestamps, initial_prompt)
        elif self.backend == WhisperBackend.FASTER_WHISPER:
            result = self._transcribe_faster_whisper(
                audio_path, word_timestamps, initial_prompt,
                temperature, beam_size
            )
        elif self.backend == WhisperBackend.OPENAI:
            result = self._transcribe_openai_whisper(
                audio_path, word_timestamps, initial_prompt,
                temperature, best_of, beam_size, patience,
                suppress_tokens, condition_on_previous_text, fp16
            )
        elif self.backend == WhisperBackend.OPENAI_API:
            result = self._transcribe_openai_api(audio_path)
        else:
            raise ValueError(f"Unknown backend: {self.backend}")

        logger.info(f"Transcription complete. Language detected: {result.get('language', 'unknown')}")

        # Clear GPU cache after transcription
        GPUMemoryManager.clear_cache()

        return result

    def _transcribe_transformers(
        self,
        audio_path: str,
        word_timestamps: bool = True,
        initial_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """Transcribe using HuggingFace Transformers (V3 Turbo, Distil-Whisper)."""
        logger.info("Using Transformers pipeline for transcription")

        # Build generate kwargs
        generate_kwargs = {}
        if self.language:
            # Map language code to model token
            generate_kwargs["language"] = self.language
        if initial_prompt:
            generate_kwargs["prompt_ids"] = self.processor.tokenizer.encode(initial_prompt)

        # Transcribe with pipeline
        result = self.pipe(
            audio_path,
            generate_kwargs=generate_kwargs,
            return_timestamps="word" if word_timestamps else True
        )

        # Convert to standard format
        formatted_result = {
            "text": result["text"],
            "language": self.language or "en",
            "segments": []
        }

        # Process chunks with word timestamps
        if "chunks" in result:
            for chunk in result["chunks"]:
                segment = {
                    "start": chunk["timestamp"][0] if chunk["timestamp"][0] is not None else 0.0,
                    "end": chunk["timestamp"][1] if chunk["timestamp"][1] is not None else 0.0,
                    "text": chunk["text"],
                    "words": []
                }

                # Add word-level timestamps if available
                if word_timestamps:
                    # For transformers, we get word boundaries in the chunk
                    words_list = chunk["text"].split()
                    if len(words_list) > 0:
                        duration = segment["end"] - segment["start"]
                        word_duration = duration / len(words_list) if len(words_list) > 0 else 0

                        for i, word_text in enumerate(words_list):
                            word = {
                                "word": word_text,
                                "start": segment["start"] + (i * word_duration),
                                "end": segment["start"] + ((i + 1) * word_duration),
                                "probability": 1.0  # Transformers doesn't provide probabilities
                            }
                            segment["words"].append(word)

                formatted_result["segments"].append(segment)

        return formatted_result

    def _transcribe_faster_whisper(
        self,
        audio_path: str,
        word_timestamps: bool = True,
        initial_prompt: Optional[str] = None,
        temperature: float = 0.0,
        beam_size: int = 5
    ) -> Dict[str, Any]:
        """Transcribe using Faster-Whisper (CTranslate2 optimization)."""
        logger.info("Using Faster-Whisper for transcription")

        # Transcribe with faster-whisper
        segments, info = self.model.transcribe(
            audio_path,
            language=self.language,
            word_timestamps=word_timestamps,
            task="transcribe",
            beam_size=beam_size,
            temperature=temperature,
            initial_prompt=initial_prompt,
            vad_filter=True,  # Voice activity detection for better accuracy
            vad_parameters=dict(min_silence_duration_ms=500)
        )

        # Convert to standard format
        result = {
            "text": "",
            "language": info.language,
            "segments": []
        }

        for segment in segments:
            segment_dict = {
                "start": segment.start,
                "end": segment.end,
                "text": segment.text,
                "words": []
            }

            # Add word-level timestamps
            if word_timestamps and hasattr(segment, 'words'):
                for word in segment.words:
                    word_dict = {
                        "word": word.word,
                        "start": word.start,
                        "end": word.end,
                        "probability": word.probability
                    }
                    segment_dict["words"].append(word_dict)

            result["segments"].append(segment_dict)
            result["text"] += segment.text + " "

        result["text"] = result["text"].strip()

        return result

    def _transcribe_openai_whisper(
        self,
        audio_path: str,
        word_timestamps: bool = True,
        initial_prompt: Optional[str] = None,
        temperature: float = 0.0,
        best_of: int = 5,
        beam_size: int = 5,
        patience: float = 1.0,
        suppress_tokens: Optional[List[int]] = None,
        condition_on_previous_text: bool = True,
        fp16: bool = True
    ) -> Dict[str, Any]:
        """Transcribe using original OpenAI Whisper."""
        logger.info("Using original OpenAI Whisper for transcription")

        result = self.model.transcribe(
            audio_path,
            language=self.language,
            word_timestamps=word_timestamps,
            task="transcribe",
            temperature=temperature,
            best_of=best_of,
            beam_size=beam_size,
            patience=patience,
            suppress_tokens=suppress_tokens,
            condition_on_previous_text=condition_on_previous_text,
            fp16=fp16,
            initial_prompt=initial_prompt
        )

        return result

    def _transcribe_openai_api(self, audio_path: str) -> Dict[str, Any]:
        """Transcribe using OpenAI Cloud API."""
        logger.info("Using OpenAI API for cloud transcription")

        with open(audio_path, "rb") as audio_file:
            transcript = openai.Audio.transcribe(
                model="whisper-1",
                file=audio_file,
                response_format="verbose_json",
                language=self.language,
                timestamp_granularities=["word", "segment"]
            )

        # Convert API response to standard format
        result = {
            "text": transcript.get("text", ""),
            "language": transcript.get("language", self.language or "en"),
            "segments": []
        }

        # Process segments
        if "segments" in transcript:
            for segment in transcript["segments"]:
                segment_dict = {
                    "start": segment.get("start", 0.0),
                    "end": segment.get("end", 0.0),
                    "text": segment.get("text", ""),
                    "words": []
                }

                # Add word-level timestamps if available
                if "words" in segment:
                    for word in segment["words"]:
                        word_dict = {
                            "word": word.get("word", ""),
                            "start": word.get("start", 0.0),
                            "end": word.get("end", 0.0),
                            "probability": 1.0
                        }
                        segment_dict["words"].append(word_dict)

                result["segments"].append(segment_dict)

        return result

    async def transcribe_realtime(
        self,
        audio_path: str,
        chunk_duration: float = 10.0
    ) -> AsyncIterator[Dict[str, Any]]:
        """
        Real-time streaming transcription.
        Yields transcription results as they become available.

        Args:
            audio_path: Path to audio/video file
            chunk_duration: Duration of each chunk in seconds

        Yields:
            Transcription results for each chunk
        """
        logger.info(f"Starting real-time transcription: {audio_path}")

        # This is a simplified implementation
        # In production, you'd stream audio chunks as they're recorded

        # For now, split the audio into chunks and process sequentially
        # This demonstrates the interface for real-time transcription

        try:
            # Process in chunks
            import librosa
            import soundfile as sf

            # Load audio
            audio, sr = librosa.load(audio_path, sr=16000)
            duration = len(audio) / sr

            # Process chunks
            chunk_samples = int(chunk_duration * sr)
            num_chunks = int(np.ceil(len(audio) / chunk_samples))

            for i in range(num_chunks):
                start_sample = i * chunk_samples
                end_sample = min((i + 1) * chunk_samples, len(audio))

                chunk = audio[start_sample:end_sample]

                # Save chunk to temp file
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
                    sf.write(temp_file.name, chunk, sr)
                    temp_path = temp_file.name

                try:
                    # Transcribe chunk
                    result = self.transcribe(temp_path, word_timestamps=True)

                    # Adjust timestamps relative to full audio
                    time_offset = i * chunk_duration
                    for segment in result.get("segments", []):
                        segment["start"] += time_offset
                        segment["end"] += time_offset
                        for word in segment.get("words", []):
                            word["start"] += time_offset
                            word["end"] += time_offset

                    yield result

                finally:
                    # Clean up temp file
                    if os.path.exists(temp_path):
                        os.unlink(temp_path)

            logger.info("Real-time transcription complete")

        except Exception as e:
            logger.error(f"Real-time transcription error: {str(e)}")
            raise

    def extract_words(self, transcription_result: Dict[str, Any]) -> List[Word]:
        """
        Extract word-level timestamps from transcription result.

        Args:
            transcription_result: Result from transcribe()

        Returns:
            List of Word objects with timing information
        """
        words = []

        for segment in transcription_result.get("segments", []):
            for word_info in segment.get("words", []):
                word = Word(
                    text=word_info["word"].strip(),
                    start=word_info["start"],
                    end=word_info["end"],
                    confidence=word_info.get("probability", 1.0)
                )
                words.append(word)

        return words


class SpeakerDiarization:
    """
    Speaker diarization using pyannote.audio.
    Identifies who is speaking when.
    """

    def __init__(self, hf_token: Optional[str] = None):
        """
        Initialize speaker diarization.

        Args:
            hf_token: HuggingFace token for accessing pyannote models
        """
        if not DIARIZATION_AVAILABLE:
            raise ImportError(
                "pyannote.audio not available. Install with: "
                "pip install pyannote.audio"
            )

        self.hf_token = hf_token or os.environ.get("HF_TOKEN")

        if not self.hf_token:
            logger.warning(
                "No HuggingFace token provided. "
                "Speaker diarization may not work."
            )

        logger.info("Loading speaker diarization pipeline")
        self.pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=self.hf_token
        )

        # Use GPU if available
        if torch.cuda.is_available():
            self.pipeline.to(torch.device("cuda"))

    def diarize(self, audio_path: str, num_speakers: Optional[int] = None) -> List[Dict]:
        """
        Perform speaker diarization on audio.

        Args:
            audio_path: Path to audio file
            num_speakers: Expected number of speakers (optional)

        Returns:
            List of speaker segments with timing
        """
        logger.info(f"Performing speaker diarization: {audio_path}")

        # Run diarization
        diarization = self.pipeline(
            audio_path,
            num_speakers=num_speakers
        )

        # Convert to list of segments
        segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker
            })

        logger.info(f"Diarization complete. Found {len(set(s['speaker'] for s in segments))} speakers")

        return segments

    @staticmethod
    def assign_speakers_to_words(
        words: List[Word],
        speaker_segments: List[Dict]
    ) -> List[Word]:
        """
        Assign speaker labels to words based on diarization.

        Args:
            words: List of Word objects
            speaker_segments: Speaker diarization segments

        Returns:
            Words with speaker labels assigned
        """
        for word in words:
            word_time = (word.start + word.end) / 2

            # Find which speaker segment this word belongs to
            for segment in speaker_segments:
                if segment["start"] <= word_time <= segment["end"]:
                    word.speaker = segment["speaker"]
                    break

        return words


class CaptionGenerator:
    """
    Generate captions from transcribed words.
    """

    @staticmethod
    def create_captions(
        words: List[Word],
        max_words_per_caption: int = 6,
        max_chars_per_caption: int = 40,
        min_duration: float = 1.0,
        max_duration: float = 5.0
    ) -> List[Caption]:
        """
        Create caption segments from words with optimal timing.

        Args:
            words: List of Word objects
            max_words_per_caption: Maximum words per caption
            max_chars_per_caption: Maximum characters per caption
            min_duration: Minimum caption display duration
            max_duration: Maximum caption display duration

        Returns:
            List of Caption objects
        """
        if not words:
            return []

        captions = []
        current_words = []
        current_chars = 0

        for word in words:
            word_len = len(word.text)

            # Check if we should start a new caption
            should_break = (
                len(current_words) >= max_words_per_caption or
                current_chars + word_len > max_chars_per_caption or
                (current_words and word.speaker != current_words[0].speaker)
            )

            if should_break and current_words:
                # Create caption from current words
                caption = CaptionGenerator._create_caption_from_words(
                    current_words,
                    min_duration,
                    max_duration
                )
                captions.append(caption)
                current_words = []
                current_chars = 0

            current_words.append(word)
            current_chars += word_len + 1  # +1 for space

        # Add remaining words
        if current_words:
            caption = CaptionGenerator._create_caption_from_words(
                current_words,
                min_duration,
                max_duration
            )
            captions.append(caption)

        return captions

    @staticmethod
    def _create_caption_from_words(
        words: List[Word],
        min_duration: float,
        max_duration: float
    ) -> Caption:
        """Create a single caption from a list of words."""
        text = " ".join(word.text for word in words)
        start = words[0].start
        end = words[-1].end

        # Optimize timing
        duration = end - start
        if duration < min_duration:
            end = start + min_duration
        elif duration > max_duration:
            end = start + max_duration

        speaker = words[0].speaker if words else None

        return Caption(
            text=text,
            start=start,
            end=end,
            words=words,
            speaker=speaker
        )


class SubtitleExporter:
    """
    Export captions to SRT and VTT formats.
    """

    @staticmethod
    def to_srt(captions: List[Caption], output_path: str) -> str:
        """
        Export captions to SRT format.

        Args:
            captions: List of Caption objects
            output_path: Path to save SRT file

        Returns:
            Path to saved SRT file
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            for i, caption in enumerate(captions, 1):
                # Write caption number
                f.write(f"{i}\n")

                # Write timing
                start_time = SubtitleExporter._format_srt_time(caption.start)
                end_time = SubtitleExporter._format_srt_time(caption.end)
                f.write(f"{start_time} --> {end_time}\n")

                # Write text
                f.write(f"{caption.text}\n\n")

        logger.info(f"SRT file saved: {output_path}")
        return output_path

    @staticmethod
    def to_vtt(captions: List[Caption], output_path: str) -> str:
        """
        Export captions to WebVTT format.

        Args:
            captions: List of Caption objects
            output_path: Path to save VTT file

        Returns:
            Path to saved VTT file
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            # Write VTT header
            f.write("WEBVTT\n\n")

            for i, caption in enumerate(captions, 1):
                # Write timing
                start_time = SubtitleExporter._format_vtt_time(caption.start)
                end_time = SubtitleExporter._format_vtt_time(caption.end)
                f.write(f"{start_time} --> {end_time}\n")

                # Write text
                f.write(f"{caption.text}\n\n")

        logger.info(f"VTT file saved: {output_path}")
        return output_path

    @staticmethod
    def _format_srt_time(seconds: float) -> str:
        """Format time for SRT (HH:MM:SS,mmm)."""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

    @staticmethod
    def _format_vtt_time(seconds: float) -> str:
        """Format time for VTT (HH:MM:SS.mmm)."""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}.{millis:03d}"


class FFmpegCaptionBurner:
    """
    Burn captions into video using FFmpeg with different styles.
    """

    def __init__(self, style_config: Optional[CaptionStyleConfig] = None):
        """
        Initialize caption burner.

        Args:
            style_config: Caption styling configuration
        """
        self.style_config = style_config or CaptionStyleConfig()

    def burn_captions_instagram(
        self,
        video_path: str,
        captions: List[Caption],
        output_path: str
    ) -> str:
        """
        Instagram style: Word-by-word pop with color highlight.

        Args:
            video_path: Input video path
            captions: List of Caption objects
            output_path: Output video path

        Returns:
            Path to output video
        """
        logger.info("Generating Instagram-style captions")

        # Generate drawtext filters for each word
        filters = []

        for caption in captions:
            for word in caption.words:
                # Escape text for FFmpeg
                text = self._escape_text(word.text)

                # Create animated word filter
                filter_str = (
                    f"drawtext="
                    f"fontfile=/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf:"
                    f"text='{text}':"
                    f"fontsize={self.style_config.font_size}:"
                    f"fontcolor={self.style_config.highlight_color}:"
                    f"x={self.style_config.position_x}:"
                    f"y={self.style_config.position_y}:"
                    f"borderw={self.style_config.border_width}:"
                    f"bordercolor={self.style_config.border_color}:"
                    f"enable='between(t,{word.start},{word.end})':"
                    f"box=1:"
                    f"boxcolor={self.style_config.box_color}"
                )
                filters.append(filter_str)

        # Combine all filters
        filter_complex = ",".join(filters)

        # Build FFmpeg command
        cmd = [
            "ffmpeg",
            "-i", video_path,
            "-vf", filter_complex,
            "-c:a", "copy",
            "-y",
            output_path
        ]

        return self._run_ffmpeg(cmd, output_path)

    def burn_captions_youtube(
        self,
        video_path: str,
        captions: List[Caption],
        output_path: str
    ) -> str:
        """
        YouTube style: Sentence blocks at bottom.

        Args:
            video_path: Input video path
            captions: List of Caption objects
            output_path: Output video path

        Returns:
            Path to output video
        """
        logger.info("Generating YouTube-style captions")

        filters = []

        for caption in captions:
            text = self._escape_text(caption.text)

            filter_str = (
                f"drawtext="
                f"fontfile=/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf:"
                f"text='{text}':"
                f"fontsize={self.style_config.font_size}:"
                f"fontcolor={self.style_config.font_color}:"
                f"x={self.style_config.position_x}:"
                f"y=h-th-30:"  # Bottom position
                f"borderw={self.style_config.border_width}:"
                f"bordercolor={self.style_config.border_color}:"
                f"enable='between(t,{caption.start},{caption.end})':"
                f"box=1:"
                f"boxcolor={self.style_config.box_color}"
            )
            filters.append(filter_str)

        filter_complex = ",".join(filters)

        cmd = [
            "ffmpeg",
            "-i", video_path,
            "-vf", filter_complex,
            "-c:a", "copy",
            "-y",
            output_path
        ]

        return self._run_ffmpeg(cmd, output_path)

    def burn_captions_karaoke(
        self,
        video_path: str,
        captions: List[Caption],
        output_path: str
    ) -> str:
        """
        Karaoke style: Word highlight as spoken with full sentence visible.

        Args:
            video_path: Input video path
            captions: List of Caption objects
            output_path: Output video path

        Returns:
            Path to output video
        """
        logger.info("Generating Karaoke-style captions")

        filters = []

        for caption in captions:
            # Show full caption in white
            full_text = self._escape_text(caption.text)

            base_filter = (
                f"drawtext="
                f"fontfile=/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf:"
                f"text='{full_text}':"
                f"fontsize={self.style_config.font_size}:"
                f"fontcolor={self.style_config.font_color}:"
                f"x={self.style_config.position_x}:"
                f"y={self.style_config.position_y}:"
                f"borderw={self.style_config.border_width}:"
                f"bordercolor={self.style_config.border_color}:"
                f"enable='between(t,{caption.start},{caption.end})':"
                f"box=1:"
                f"boxcolor={self.style_config.box_color}"
            )
            filters.append(base_filter)

            # Highlight each word as it's spoken
            for word in caption.words:
                word_text = self._escape_text(word.text)

                # Find word position in full text
                word_index = caption.text.find(word.text)
                if word_index == -1:
                    continue

                highlight_filter = (
                    f"drawtext="
                    f"fontfile=/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf:"
                    f"text='{word_text}':"
                    f"fontsize={self.style_config.font_size}:"
                    f"fontcolor={self.style_config.highlight_color}:"
                    f"x={self.style_config.position_x}+{word_index * 10}:"  # Approximate position
                    f"y={self.style_config.position_y}:"
                    f"enable='between(t,{word.start},{word.end})':"
                    f"alpha=0.9"
                )
                filters.append(highlight_filter)

        filter_complex = ",".join(filters)

        cmd = [
            "ffmpeg",
            "-i", video_path,
            "-vf", filter_complex,
            "-c:a", "copy",
            "-y",
            output_path
        ]

        return self._run_ffmpeg(cmd, output_path)

    def burn_captions_tiktok(
        self,
        video_path: str,
        captions: List[Caption],
        output_path: str
    ) -> str:
        """
        TikTok style: Centered, bold, animated captions.

        Args:
            video_path: Input video path
            captions: List of Caption objects
            output_path: Output video path

        Returns:
            Path to output video
        """
        logger.info("Generating TikTok-style captions")

        filters = []

        for caption in captions:
            text = self._escape_text(caption.text.upper())  # TikTok uses uppercase

            filter_str = (
                f"drawtext="
                f"fontfile=/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf:"
                f"text='{text}':"
                f"fontsize={self.style_config.font_size + 12}:"  # Larger font
                f"fontcolor={self.style_config.font_color}:"
                f"x=(w-text_w)/2:"  # Center horizontally
                f"y=(h-text_h)/2:"  # Center vertically
                f"borderw={self.style_config.border_width + 2}:"
                f"bordercolor={self.style_config.border_color}:"
                f"enable='between(t,{caption.start},{caption.end})':"
                f"box=1:"
                f"boxcolor={self.style_config.box_color}:"
                f"shadowx={self.style_config.shadow_x}:"
                f"shadowy={self.style_config.shadow_y}:"
                f"shadowcolor={self.style_config.shadow_color}"
            )
            filters.append(filter_str)

        filter_complex = ",".join(filters)

        cmd = [
            "ffmpeg",
            "-i", video_path,
            "-vf", filter_complex,
            "-c:a", "copy",
            "-y",
            output_path
        ]

        return self._run_ffmpeg(cmd, output_path)

    def burn_captions_hormozi(
        self,
        video_path: str,
        captions: List[Caption],
        output_path: str
    ) -> str:
        """
        Hormozi style: Big bold words, one at a time, centered.

        Args:
            video_path: Input video path
            captions: List of Caption objects
            output_path: Output video path

        Returns:
            Path to output video
        """
        logger.info("Generating Hormozi-style captions")

        filters = []

        # Show one word at a time, very large and centered
        for caption in captions:
            for word in caption.words:
                text = self._escape_text(word.text.upper())

                filter_str = (
                    f"drawtext="
                    f"fontfile=/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf:"
                    f"text='{text}':"
                    f"fontsize={self.style_config.font_size + 32}:"  # Much larger
                    f"fontcolor={self.style_config.highlight_color}:"
                    f"x=(w-text_w)/2:"  # Center horizontally
                    f"y=(h-text_h)/2:"  # Center vertically
                    f"borderw={self.style_config.border_width + 3}:"
                    f"bordercolor={self.style_config.border_color}:"
                    f"enable='between(t,{word.start},{word.end})':"
                    f"box=1:"
                    f"boxcolor=black@0.8:"
                    f"shadowx={self.style_config.shadow_x * 2}:"
                    f"shadowy={self.style_config.shadow_y * 2}:"
                    f"shadowcolor={self.style_config.shadow_color}"
                )
                filters.append(filter_str)

        filter_complex = ",".join(filters)

        cmd = [
            "ffmpeg",
            "-i", video_path,
            "-vf", filter_complex,
            "-c:a", "copy",
            "-y",
            output_path
        ]

        return self._run_ffmpeg(cmd, output_path)

    def burn_captions_from_srt(
        self,
        video_path: str,
        srt_path: str,
        output_path: str,
        style: CaptionStyle = CaptionStyle.YOUTUBE
    ) -> str:
        """
        Burn captions from SRT file into video.

        Args:
            video_path: Input video path
            srt_path: Path to SRT file
            output_path: Output video path
            style: Caption style to use

        Returns:
            Path to output video
        """
        logger.info(f"Burning captions from SRT: {srt_path}")

        # Basic subtitle filter
        subtitles_filter = f"subtitles={srt_path}"

        cmd = [
            "ffmpeg",
            "-i", video_path,
            "-vf", subtitles_filter,
            "-c:a", "copy",
            "-y",
            output_path
        ]

        return self._run_ffmpeg(cmd, output_path)

    @staticmethod
    def _escape_text(text: str) -> str:
        """Escape text for FFmpeg drawtext filter."""
        # Escape special characters for FFmpeg
        text = text.replace("\\", "\\\\")
        text = text.replace("'", "\\'")
        text = text.replace(":", "\\:")
        text = text.replace("%", "\\%")
        return text

    @staticmethod
    def _run_ffmpeg(cmd: List[str], output_path: str) -> str:
        """Run FFmpeg command."""
        try:
            logger.info(f"Running FFmpeg command: {' '.join(cmd)}")

            result = subprocess.run(
                cmd,
                check=True,
                capture_output=True,
                text=True
            )

            logger.info(f"Captions burned successfully: {output_path}")
            return output_path

        except subprocess.CalledProcessError as e:
            logger.error(f"FFmpeg error: {e.stderr}")
            raise


class AutoCaptionSystem:
    """
    UPGRADED: Complete auto-caption system for pro-grade video ads (2025 Edition).

    NEW FEATURES:
    - Whisper V3 Turbo (8x faster)
    - Distil-Whisper preview mode (6x faster)
    - Real-time transcription
    - Batch processing
    - Multi-language translation
    - GPU memory optimization
    """

    def __init__(
        self,
        model_size: WhisperModelSize = WhisperModelSize.LARGE_V3_TURBO,
        backend: WhisperBackend = WhisperBackend.TRANSFORMERS,
        device: Optional[str] = None,
        language: Optional[str] = None,
        mode: TranscriptionMode = TranscriptionMode.FULL,
        enable_diarization: bool = False,
        hf_token: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        enable_profanity_filter: bool = True,
        enable_fitness_vocab: bool = True,
        enable_translation: bool = False,
        target_language: Optional[str] = None
    ):
        """
        Initialize auto-caption system.

        Args:
            model_size: Whisper model size (defaults to V3 Turbo!)
            backend: Backend to use (transformers, faster, openai, api)
            device: Device to use ('cuda' or 'cpu')
            language: Language code (auto-detect if None)
            mode: Transcription mode (full, fast, realtime, batch)
            enable_diarization: Enable speaker diarization
            hf_token: HuggingFace token for diarization
            openai_api_key: OpenAI API key for cloud fallback
            enable_profanity_filter: Enable profanity filtering
            enable_fitness_vocab: Enable fitness vocabulary enhancement
            enable_translation: Enable auto-translation
            target_language: Target language for translation
        """
        # Initialize transcriber with new features
        self.transcriber = WhisperTranscriber(
            model_size=model_size,
            backend=backend,
            device=device,
            language=language,
            mode=mode,
            openai_api_key=openai_api_key
        )

        self.enable_diarization = enable_diarization and DIARIZATION_AVAILABLE
        if self.enable_diarization:
            self.diarizer = SpeakerDiarization(hf_token=hf_token)
        else:
            self.diarizer = None

        self.enable_profanity_filter = enable_profanity_filter
        self.enable_fitness_vocab = enable_fitness_vocab
        self.enable_translation = enable_translation
        self.target_language = target_language

        # Queue for batch processing
        self.queue = TranscriptionQueue(max_workers=4)

        logger.info(f"AutoCaptionSystem initialized with {model_size.value} ({backend.value})")

    def process_video(
        self,
        video_path: str,
        output_dir: Optional[str] = None,
        caption_style: CaptionStyle = CaptionStyle.HORMOZI,
        style_config: Optional[CaptionStyleConfig] = None,
        generate_srt: bool = True,
        generate_vtt: bool = True,
        burn_captions: bool = True,
        num_speakers: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Process video and generate captions.

        Args:
            video_path: Path to input video
            output_dir: Output directory (creates temp dir if None)
            caption_style: Style of captions to generate
            style_config: Caption styling configuration
            generate_srt: Generate SRT file
            generate_vtt: Generate VTT file
            burn_captions: Burn captions into video
            num_speakers: Expected number of speakers (for diarization)

        Returns:
            Dictionary with paths to generated files and transcription data
        """
        logger.info(f"Processing video: {video_path}")

        # Create output directory
        if output_dir is None:
            output_dir = tempfile.mkdtemp()
        os.makedirs(output_dir, exist_ok=True)

        video_name = Path(video_path).stem

        # Step 1: Transcribe audio
        logger.info("Step 1: Transcribing audio with Whisper")
        transcription = self.transcriber.transcribe(video_path, word_timestamps=True)

        # Step 2: Extract words
        logger.info("Step 2: Extracting word-level timestamps")
        words = self.transcriber.extract_words(transcription)

        # Step 3: Speaker diarization (optional)
        if self.enable_diarization and self.diarizer:
            logger.info("Step 3: Performing speaker diarization")
            speaker_segments = self.diarizer.diarize(video_path, num_speakers)
            words = SpeakerDiarization.assign_speakers_to_words(words, speaker_segments)
        else:
            logger.info("Step 3: Skipping speaker diarization")

        # Step 4: Process text
        logger.info("Step 4: Processing text")
        for word in words:
            # Apply fitness vocabulary
            if self.enable_fitness_vocab:
                word.text = FitnessVocabulary.enhance_text(word.text)

            # Apply profanity filter
            if self.enable_profanity_filter:
                word.text = ProfanityFilter.filter_text(word.text)

        # Step 5: Generate captions
        logger.info("Step 5: Generating caption segments")
        captions = CaptionGenerator.create_captions(words)

        # Step 6: Translation (optional)
        if self.enable_translation and self.target_language:
            source_lang = LanguageDetector.detect_language(transcription)
            if source_lang != self.target_language:
                logger.info(f"Step 6a: Translating from {source_lang} to {self.target_language}")
                captions = LanguageDetector.translate_captions(
                    captions,
                    source_lang,
                    self.target_language
                )

        # Step 7: Export to SRT/VTT
        result = {
            "video_path": video_path,
            "transcription": transcription,
            "words": [asdict(w) for w in words],
            "captions": [asdict(c) for c in captions],
            "language": transcription["language"],
            "translated": self.enable_translation and self.target_language is not None
        }

        if generate_srt:
            logger.info("Step 6a: Exporting to SRT")
            srt_path = os.path.join(output_dir, f"{video_name}.srt")
            SubtitleExporter.to_srt(captions, srt_path)
            result["srt_path"] = srt_path

        if generate_vtt:
            logger.info("Step 6b: Exporting to VTT")
            vtt_path = os.path.join(output_dir, f"{video_name}.vtt")
            SubtitleExporter.to_vtt(captions, vtt_path)
            result["vtt_path"] = vtt_path

        # Step 7: Burn captions into video
        if burn_captions:
            logger.info(f"Step 7: Burning captions into video ({caption_style.value} style)")

            burner = FFmpegCaptionBurner(style_config or CaptionStyleConfig())
            output_video_path = os.path.join(
                output_dir,
                f"{video_name}_captioned_{caption_style.value}.mp4"
            )

            # Select burn method based on style
            if caption_style == CaptionStyle.INSTAGRAM:
                burner.burn_captions_instagram(video_path, captions, output_video_path)
            elif caption_style == CaptionStyle.YOUTUBE:
                burner.burn_captions_youtube(video_path, captions, output_video_path)
            elif caption_style == CaptionStyle.KARAOKE:
                burner.burn_captions_karaoke(video_path, captions, output_video_path)
            elif caption_style == CaptionStyle.TIKTOK:
                burner.burn_captions_tiktok(video_path, captions, output_video_path)
            elif caption_style == CaptionStyle.HORMOZI:
                burner.burn_captions_hormozi(video_path, captions, output_video_path)
            else:
                # Default to SRT burn-in
                if generate_srt:
                    burner.burn_captions_from_srt(
                        video_path,
                        result["srt_path"],
                        output_video_path,
                        caption_style
                    )

            result["captioned_video_path"] = output_video_path

        logger.info(f"Processing complete! Output saved to: {output_dir}")

        return result

    def process_video_fast_preview(
        self,
        video_path: str,
        output_dir: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        NEW: Fast preview mode using Distil-Whisper for quick transcription.
        6x faster than standard mode, good for previews.

        Args:
            video_path: Path to input video
            output_dir: Output directory

        Returns:
            Dictionary with transcription results
        """
        logger.info(f"FAST PREVIEW MODE: Processing {video_path}")

        # Create temporary transcriber with Distil-Whisper
        preview_transcriber = WhisperTranscriber(
            model_size=WhisperModelSize.DISTIL_LARGE_V2,
            backend=WhisperBackend.TRANSFORMERS,
            device=self.transcriber.device,
            language=self.transcriber.language
        )

        # Transcribe with Distil-Whisper (much faster!)
        transcription = preview_transcriber.transcribe(video_path, word_timestamps=True)

        # Extract words and generate captions
        words = preview_transcriber.extract_words(transcription)
        captions = CaptionGenerator.create_captions(words)

        # Create output directory
        if output_dir is None:
            output_dir = tempfile.mkdtemp()
        os.makedirs(output_dir, exist_ok=True)

        video_name = Path(video_path).stem

        # Export to SRT
        srt_path = os.path.join(output_dir, f"{video_name}_preview.srt")
        SubtitleExporter.to_srt(captions, srt_path)

        result = {
            "video_path": video_path,
            "transcription": transcription,
            "words": [asdict(w) for w in words],
            "captions": [asdict(c) for c in captions],
            "language": transcription["language"],
            "srt_path": srt_path,
            "mode": "preview"
        }

        logger.info("Fast preview complete!")

        # Clean up
        GPUMemoryManager.clear_cache()

        return result

    async def process_video_realtime(
        self,
        video_path: str,
        output_dir: Optional[str] = None,
        callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        NEW: Real-time transcription with streaming results.
        Process video and yield results as they become available.

        Args:
            video_path: Path to input video
            output_dir: Output directory
            callback: Optional callback function to receive streaming results

        Returns:
            Dictionary with final transcription results
        """
        logger.info(f"REALTIME MODE: Processing {video_path}")

        all_words = []
        all_segments = []

        # Stream transcription
        async for chunk_result in self.transcriber.transcribe_realtime(video_path):
            # Extract words from chunk
            chunk_words = self.transcriber.extract_words(chunk_result)
            all_words.extend(chunk_words)

            # Add segments
            all_segments.extend(chunk_result.get("segments", []))

            # Call callback if provided
            if callback:
                await callback({
                    "chunk_words": [asdict(w) for w in chunk_words],
                    "progress": len(all_words)
                })

            logger.info(f"Processed chunk: {len(chunk_words)} words")

        # Generate final captions
        captions = CaptionGenerator.create_captions(all_words)

        # Create output directory
        if output_dir is None:
            output_dir = tempfile.mkdtemp()
        os.makedirs(output_dir, exist_ok=True)

        video_name = Path(video_path).stem

        # Export to SRT
        srt_path = os.path.join(output_dir, f"{video_name}_realtime.srt")
        SubtitleExporter.to_srt(captions, srt_path)

        result = {
            "video_path": video_path,
            "words": [asdict(w) for w in all_words],
            "captions": [asdict(c) for c in captions],
            "srt_path": srt_path,
            "mode": "realtime"
        }

        logger.info("Real-time transcription complete!")

        return result

    def process_batch(
        self,
        video_paths: List[str],
        output_dir: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Dict[str, Any]]:
        """
        NEW: Batch process multiple videos efficiently.
        Uses queue management and optimized GPU memory.

        Args:
            video_paths: List of video paths to process
            output_dir: Output directory
            **kwargs: Additional arguments for process_video

        Returns:
            Dictionary mapping video paths to results
        """
        logger.info(f"BATCH MODE: Processing {len(video_paths)} videos")

        # Add jobs to queue
        job_ids = {}
        for video_path in video_paths:
            job_id = f"job_{Path(video_path).stem}"
            self.queue.add_job(job_id, video_path, **kwargs)
            job_ids[video_path] = job_id

        # Process queue
        self.queue.process_queue(self.transcriber)

        # Collect results
        results = {}
        for video_path, job_id in job_ids.items():
            job_result = self.queue.get_result(job_id)
            if job_result and job_result["status"] == "completed":
                # Process the transcription result
                transcription = job_result["result"]
                words = self.transcriber.extract_words(transcription)
                captions = CaptionGenerator.create_captions(words)

                # Create output
                if output_dir:
                    os.makedirs(output_dir, exist_ok=True)
                    video_name = Path(video_path).stem
                    srt_path = os.path.join(output_dir, f"{video_name}.srt")
                    SubtitleExporter.to_srt(captions, srt_path)

                    results[video_path] = {
                        "status": "success",
                        "transcription": transcription,
                        "words": [asdict(w) for w in words],
                        "captions": [asdict(c) for c in captions],
                        "srt_path": srt_path
                    }
                else:
                    results[video_path] = {
                        "status": "success",
                        "transcription": transcription,
                        "words": [asdict(w) for w in words],
                        "captions": [asdict(c) for c in captions]
                    }
            else:
                results[video_path] = {
                    "status": "failed",
                    "error": job_result.get("error", "Unknown error") if job_result else "No result"
                }

        logger.info(f"Batch processing complete! Processed {len(results)} videos")

        # Clean up GPU memory
        GPUMemoryManager.clear_cache()

        return results

    def create_translated_captions(
        self,
        video_path: str,
        source_language: str,
        target_languages: List[str],
        output_dir: Optional[str] = None
    ) -> Dict[str, str]:
        """
        NEW: Create captions in multiple languages from one transcription.
        Uses Whisper + translation for multi-language support.

        Args:
            video_path: Path to input video
            source_language: Source language code
            target_languages: List of target language codes
            output_dir: Output directory

        Returns:
            Dictionary mapping language codes to SRT paths
        """
        logger.info(f"Creating multi-language captions: {target_languages}")

        # Transcribe in source language
        self.transcriber.language = source_language
        transcription = self.transcriber.transcribe(video_path, word_timestamps=True)

        # Extract words and generate captions
        words = self.transcriber.extract_words(transcription)
        source_captions = CaptionGenerator.create_captions(words)

        # Create output directory
        if output_dir is None:
            output_dir = tempfile.mkdtemp()
        os.makedirs(output_dir, exist_ok=True)

        video_name = Path(video_path).stem

        # Export source language
        srt_paths = {}
        source_srt = os.path.join(output_dir, f"{video_name}_{source_language}.srt")
        SubtitleExporter.to_srt(source_captions, source_srt)
        srt_paths[source_language] = source_srt

        # Translate to target languages
        for target_lang in target_languages:
            if target_lang == source_language:
                continue

            logger.info(f"Translating to {target_lang}")

            translated_captions = LanguageDetector.translate_captions(
                source_captions,
                source_language,
                target_lang
            )

            target_srt = os.path.join(output_dir, f"{video_name}_{target_lang}.srt")
            SubtitleExporter.to_srt(translated_captions, target_srt)
            srt_paths[target_lang] = target_srt

        logger.info(f"Multi-language captions created: {list(srt_paths.keys())}")

        return srt_paths


# Example usage and utility functions

def get_available_whisper_models() -> List[str]:
    """Get list of available Whisper models."""
    return [model.value for model in WhisperModelSize]


def estimate_processing_time(
    video_duration_seconds: float,
    model_size: WhisperModelSize,
    has_gpu: bool,
    backend: WhisperBackend = WhisperBackend.TRANSFORMERS
) -> float:
    """
    UPDATED: Estimate processing time for a video (November 2025).

    Args:
        video_duration_seconds: Duration of video in seconds
        model_size: Whisper model size
        has_gpu: Whether GPU is available
        backend: Backend to use

    Returns:
        Estimated processing time in seconds
    """
    # Updated speed factors for 2025 models (November 2025)
    speed_factors = {
        WhisperModelSize.TINY: 0.05 if has_gpu else 0.2,
        WhisperModelSize.BASE: 0.1 if has_gpu else 0.4,
        WhisperModelSize.SMALL: 0.2 if has_gpu else 0.8,
        WhisperModelSize.MEDIUM: 0.4 if has_gpu else 1.6,
        WhisperModelSize.LARGE: 0.8 if has_gpu else 3.2,
        WhisperModelSize.LARGE_V2: 0.8 if has_gpu else 3.2,
        WhisperModelSize.LARGE_V3: 0.8 if has_gpu else 3.2,

        # NEW: V3 Turbo is 8x faster than Large V3!
        WhisperModelSize.LARGE_V3_TURBO: 0.1 if has_gpu else 0.8,

        # NEW: Distil-Whisper is 6x faster!
        WhisperModelSize.DISTIL_LARGE_V2: 0.13 if has_gpu else 0.5,
        WhisperModelSize.DISTIL_MEDIUM_EN: 0.08 if has_gpu else 0.3,
        WhisperModelSize.DISTIL_SMALL_EN: 0.05 if has_gpu else 0.2,
    }

    # Faster-Whisper backend is 2x faster
    if backend == WhisperBackend.FASTER_WHISPER:
        base_factor = speed_factors.get(model_size, 1.0) * 0.5
    # Transformers is optimized
    elif backend == WhisperBackend.TRANSFORMERS:
        base_factor = speed_factors.get(model_size, 1.0)
    # Original Whisper is slower
    elif backend == WhisperBackend.OPENAI:
        base_factor = speed_factors.get(model_size, 1.0) * 1.2
    # API depends on network
    elif backend == WhisperBackend.OPENAI_API:
        base_factor = 0.3  # Usually fast, but depends on API
    else:
        base_factor = speed_factors.get(model_size, 1.0)

    return video_duration_seconds * base_factor


def compare_model_speeds() -> str:
    """
    NEW: Compare speed improvements of 2025 models.

    Returns:
        Comparison table as string
    """
    comparison = """
    WHISPER MODEL SPEED COMPARISON (November 2025)
    ================================================

    For 60-second video with GPU:

    Model                   | Time (sec) | Speedup vs Large V3
    ---------------------------------------------------------
    Large V3                |    48s     | 1x (baseline)
    Large V3 TURBO (NEW!)   |     6s     | 8x FASTER 
    Distil-Large-V2 (NEW!)  |     8s     | 6x FASTER 
    Distil-Medium-EN (NEW!) |     5s     | 9.6x FASTER 

    BACKEND COMPARISON:
    ---------------------------------------------------------
    Transformers            | Optimal for V3 Turbo & Distil
    Faster-Whisper          | 2x faster for standard models
    OpenAI API              | No GPU needed, cloud-based

    RECOMMENDED CONFIGURATIONS:
    ---------------------------------------------------------
    Production (accuracy)   : Large V3 Turbo + Transformers
    Preview (speed)         : Distil-Medium-EN + Transformers
    No GPU (cloud)          : OpenAI API
    Batch (efficiency)      : Faster-Whisper + Large V3
    """
    return comparison


def main():
    """UPDATED: Example usage of the auto-caption system (2025 Edition)."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Auto-caption system for video ads (2025 Edition - Whisper V3 Turbo)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=compare_model_speeds()
    )

    # Required arguments
    parser.add_argument("video_path", help="Path to input video")

    # Output settings
    parser.add_argument("--output-dir", help="Output directory")

    # NEW: Model and backend selection
    parser.add_argument(
        "--model",
        choices=[m.value for m in WhisperModelSize],
        default="large-v3-turbo",  # NEW DEFAULT: V3 Turbo!
        help="Whisper model size (default: large-v3-turbo)"
    )
    parser.add_argument(
        "--backend",
        choices=[b.value for b in WhisperBackend],
        default="transformers",
        help="Backend to use (default: transformers)"
    )

    # NEW: Processing modes
    parser.add_argument(
        "--mode",
        choices=["full", "fast", "realtime", "batch"],
        default="full",
        help="Processing mode: full (accuracy), fast (preview), realtime (streaming), batch (multiple)"
    )

    # Caption style
    parser.add_argument(
        "--style",
        choices=[s.value for s in CaptionStyle],
        default="hormozi",
        help="Caption style"
    )

    # Language settings
    parser.add_argument("--language", help="Language code (e.g., en, es)")
    parser.add_argument("--translate-to", help="Translate captions to this language")

    # Advanced features
    parser.add_argument("--diarization", action="store_true", help="Enable speaker diarization")
    parser.add_argument("--hf-token", help="HuggingFace token for diarization")
    parser.add_argument("--openai-api-key", help="OpenAI API key for cloud fallback")

    # Output options
    parser.add_argument("--no-burn", action="store_true", help="Skip burning captions")
    parser.add_argument("--font-size", type=int, default=48, help="Font size")
    parser.add_argument("--font-color", default="white", help="Font color")
    parser.add_argument("--highlight-color", default="yellow", help="Highlight color")

    # NEW: Batch processing
    parser.add_argument("--batch-videos", nargs="+", help="Process multiple videos in batch mode")

    # NEW: Show speed comparison
    parser.add_argument("--show-speeds", action="store_true", help="Show model speed comparison and exit")

    args = parser.parse_args()

    # Show speed comparison if requested
    if args.show_speeds:
        print(compare_model_speeds())
        return

    # Create style config
    style_config = CaptionStyleConfig(
        font_size=args.font_size,
        font_color=args.font_color,
        highlight_color=args.highlight_color
    )

    # Initialize system with NEW features
    system = AutoCaptionSystem(
        model_size=WhisperModelSize(args.model),
        backend=WhisperBackend(args.backend),
        language=args.language,
        mode=TranscriptionMode(args.mode.upper()) if args.mode != "batch" else TranscriptionMode.FULL,
        enable_diarization=args.diarization,
        hf_token=args.hf_token,
        openai_api_key=args.openai_api_key,
        enable_translation=args.translate_to is not None,
        target_language=args.translate_to
    )

    print("\n" + "="*60)
    print("AUTO-CAPTION SYSTEM (2025 Edition)")
    print("="*60)
    print(f"Model: {args.model}")
    print(f"Backend: {args.backend}")
    print(f"Mode: {args.mode}")
    print(f"GPU Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        gpu_mem = GPUMemoryManager.get_available_memory()
        print(f"GPU Memory: {gpu_mem:.1f}GB available")
    print("="*60 + "\n")

    # Handle different modes
    if args.mode == "fast":
        # Fast preview mode
        print("Using FAST PREVIEW mode (Distil-Whisper)")
        result = system.process_video_fast_preview(
            video_path=args.video_path,
            output_dir=args.output_dir
        )

    elif args.mode == "realtime":
        # Real-time mode
        print("Using REALTIME mode (streaming transcription)")
        import asyncio

        async def progress_callback(data):
            print(f"Progress: {data['progress']} words processed...")

        result = asyncio.run(system.process_video_realtime(
            video_path=args.video_path,
            output_dir=args.output_dir,
            callback=progress_callback
        ))

    elif args.mode == "batch" or args.batch_videos:
        # Batch mode
        video_paths = args.batch_videos if args.batch_videos else [args.video_path]
        print(f"Using BATCH mode: Processing {len(video_paths)} videos")

        results = system.process_batch(
            video_paths=video_paths,
            output_dir=args.output_dir
        )

        # Print batch results
        print("\n" + "="*60)
        print("BATCH PROCESSING COMPLETE")
        print("="*60)
        for video_path, result in results.items():
            print(f"\nVideo: {video_path}")
            print(f"Status: {result['status']}")
            if result['status'] == "success":
                print(f"  Words: {len(result['words'])}")
                print(f"  Captions: {len(result['captions'])}")
                if 'srt_path' in result:
                    print(f"  SRT: {result['srt_path']}")
            else:
                print(f"  Error: {result.get('error', 'Unknown')}")
        print("="*60 + "\n")
        return

    else:
        # Full mode (standard processing)
        print("Using FULL mode (maximum accuracy)")
        result = system.process_video(
            video_path=args.video_path,
            output_dir=args.output_dir,
            caption_style=CaptionStyle(args.style),
            style_config=style_config,
            burn_captions=not args.no_burn
        )

    # Print results
    print("\n" + "="*60)
    print("AUTO-CAPTION PROCESSING COMPLETE")
    print("="*60)
    print(f"Model: {args.model} ({args.backend})")
    print(f"Mode: {args.mode}")
    print(f"Language detected: {result.get('language', 'unknown')}")
    print(f"Total words: {len(result['words'])}")
    print(f"Total captions: {len(result['captions'])}")

    if result.get('translated'):
        print(f"Translated to: {args.translate_to}")

    if "srt_path" in result:
        print(f"SRT file: {result['srt_path']}")

    if "vtt_path" in result:
        print(f"VTT file: {result['vtt_path']}")

    if "captioned_video_path" in result:
        print(f"Captioned video: {result['captioned_video_path']}")

    print("="*60 + "\n")

    # Show performance stats
    if torch.cuda.is_available():
        print("GPU Memory after processing:")
        print(f"  Available: {GPUMemoryManager.get_available_memory():.1f}GB")
        print()


if __name__ == "__main__":
    main()
